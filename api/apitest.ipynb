{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Imran\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mlflow\\tracking\\_tracking_service\\utils.py:140: FutureWarning: Filesystem tracking backend (e.g., './mlruns') is deprecated. Please switch to a database backend (e.g., 'sqlite:///mlflow.db'). For feedback, see: https://github.com/mlflow/mlflow/issues/18534\n",
      "  return FileStore(store_uri, store_uri)\n",
      "c:\\Users\\Imran\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mlflow\\tracking\\_model_registry\\utils.py:215: FutureWarning: Filesystem model registry backend (e.g., './mlruns') is deprecated. Please switch to a database backend (e.g., 'sqlite:///mlflow.db'). For feedback, see: https://github.com/mlflow/mlflow/issues/18534\n",
      "  return FileStore(store_uri)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur de chargement du modèle: Registered Model with name=iris_classifier not found\n",
      "Prometheus metrics server started on port 8000\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:8080\n",
      " * Running on http://192.168.1.72:8080\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mlflow.pyfunc\n",
    "from flask import Flask, request, jsonify\n",
    "from prometheus_client import start_http_server, Counter, Summary, Histogram\n",
    "\n",
    "# Configuration et initialisation \n",
    "# Le nom du modèle enregistré dans le MLflow Model Registry (e.g., 'iris_random_forest')\n",
    "MODEL_NAME = os.environ.get(\"MODEL_NAME\", \"iris_classifier\") \n",
    "# L'étape de vie à charger (e.g., 'Production')\n",
    "MODEL_STAGE = os.environ.get(\"MODEL_STAGE\", \"Production\") \n",
    "# Le chemin vers le serveur de tracking (mlruns/) si local, ou l'URL distante\n",
    "MLFLOW_TRACKING_URI = os.environ.get(\"MLFLOW_TRACKING_URI\", \"file:///app/mlruns\") \n",
    "\n",
    "# --- MLflow Model Loading ---\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "# L'URL MLflow pour charger la version \"Production\" du modèle\n",
    "MODEL_URI = f\"models:/{MODEL_NAME}/{MODEL_STAGE}\" \n",
    "\n",
    "try:\n",
    "    model = mlflow.pyfunc.load_model(MODEL_URI)\n",
    "except Exception as e:\n",
    "    print(f\"Erreur de chargement du modèle: {e}\")\n",
    "    model = None # Fallback ou gestion d'erreur\n",
    "\n",
    "# --- Monitoring Prometheus ---\n",
    "REQUEST_COUNT = Counter(\n",
    "    'http_requests_total', 'Total HTTP Requests', ['method', 'endpoint']\n",
    ")\n",
    "REQUEST_LATENCY = Histogram(\n",
    "    'http_request_latency_seconds', 'HTTP Request Latency', ['method', 'endpoint']\n",
    ")\n",
    "\n",
    "# --- Flask App ---\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "@REQUEST_LATENCY.time()\n",
    "def predict():\n",
    "    REQUEST_COUNT.labels(method='post', endpoint='/predict').inc()\n",
    "    \n",
    "    if model is None:\n",
    "        return jsonify({\"error\": \"Model not loaded\"}), 500\n",
    "\n",
    "    try:\n",
    "        data = request.get_json(force=True)\n",
    "        \n",
    "        # Supposons que 'data' est une liste de features, \n",
    "        # le modèle prend en charge la prédiction\n",
    "        prediction = model.predict(data.get('features'))\n",
    "\n",
    "        return jsonify({\"prediction\": prediction.tolist()})\n",
    "\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 400\n",
    "\n",
    "# Point de terminaison pour les métriques, utilisé par Prometheus\n",
    "@app.route('/metrics')\n",
    "def metrics():\n",
    "    from prometheus_client import generate_latest, CONTENT_TYPE_LATEST\n",
    "    return generate_latest(), 200, {'Content-Type': CONTENT_TYPE_LATEST}\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Démarrer le serveur de métriques Prometheus sur un port séparé (e.g., 8000)\n",
    "    start_http_server(8000) \n",
    "    print(\"Prometheus metrics server started on port 8000\")\n",
    "    \n",
    "    # Lancer l'API Flask\n",
    "    app.run(host='0.0.0.0', port=8080)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
